{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_Instance_segmentation.ipynb","provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyMAQHAdCsBUExhHlkv2hk3r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[" < read the notes with the numbering order for better understanding :D >\n","\n","1. This is an instance segmentation based on a pretrained model on Mask-RCNN to segmentate \"Chest-CT Scan \" images to Normal=Background , GGO=Ground Glass Opacity=Red , and C=Consolidation=Blue , which are the most common lesions seen in Chest CT Scan images. The goal of this code is to Mask these Lesions over these images and show masks and the probabalities of them\n","\n","2. In order to start, lets prepare the colab environment :"],"metadata":{"id":"1USsKPhJ4VcA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8VBe-h-GCMS"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/Colab Notebooks/CVFOLDER/CVPROJECT     #path to the folder\n","import sys\n","path_to_module = '/content/gdrive/MyDrive/Colab Notebooks/CVFOLDER/CVPROJECT'  # path to folder to import libs.\n","sys.path.append(path_to_module)"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"iHTVwFeA6V6V"}},{"cell_type":"markdown","source":["3. then importing libraries needed for sure ðŸ§‘"],"metadata":{"id":"OpCPBCBI4fqF"}},{"cell_type":"code","source":["\n","import models.mask_net as mask_net\n","from models.mask_net.rpn_segmentation import AnchorGenerator\n","import time\n","import copy\n","import torch       \n","import torchvision\n","import numpy as np\n","import os\n","import cv2\n","from models.mask_net.faster_rcnn import FastRCNNPredictor, TwoMLPHead\n","from models.mask_net.covid_mask_net import MaskRCNNHeads, MaskRCNNPredictor\n","from torchvision import transforms\n","from PIL import Image as PILImage\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","from tqdm import tqdm"],"metadata":{"id":"g5tnX7NZxLLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Now its time to get the information of config dict and load the pretrained model, prepare Mask Rcnn  model and do segmentation task these information in config dict are actually hyper parameters which will be explained in last step"],"metadata":{"id":"zoGynUjDNgK4"}},{"cell_type":"code","source":["# main method\n","def main(config):\n","    if config[\"device\"] == 'cuda' and torch.cuda.is_available():\n","        device = torch.device('cuda')\n","    else:\n","        device = torch.device('cpu')\n","    confidence_threshold = config[\"confidence_th\"]\n","    mask_threshold = config[\"mask_logits_th\"]\n","    save_dir = config[\"save_dir\"]         # where saves segmented imgs \n","    data_dir = config[\"test_data_dir\"]    # path to test folder\n","    img_dir =  config[\"test_img_dir\"]     #path to imgs in test folder\n","    mask_type = config[\"mask_type\"]\n","    rpn_nms = config[\"rpn_nms_th\"]\n","    roi_nms =  config[\"roi_nms_th\"]\n","    truncation = config[\"truncation\"]\n","    backbone_name='resnet50'\n","    if mask_type == \"both\":\n","        n_c = 3\n","    else:\n","        n_c = 2\n","    ckpt = torch.load(config[\"ckpt\"], map_location=device)   #loads pretrained model( .pth) file\n","\n","    sizes = ckpt['anchor_generator'].sizes\n","    aspect_ratios = ckpt['anchor_generator'].aspect_ratios\n","    anchor_generator = AnchorGenerator(sizes, aspect_ratios)\n","    print(\"Anchors: \", anchor_generator.sizes, anchor_generator.aspect_ratios)\n","\n","    box_head = TwoMLPHead(in_channels=7 * 7 * 256, representation_size=128)\n","    box_predictor = FastRCNNPredictor(in_channels=128, num_classes=n_c)\n","    mask_predictor = MaskRCNNPredictor(in_channels=256, dim_reduced=256, num_classes=n_c)\n","\n","    # keyword arguments\n","    maskrcnn_args = {'num_classes': None, 'min_size': 512, 'max_size': 1024, 'box_detections_per_img': 100,\n","                     'box_nms_thresh': roi_nms, 'box_score_thresh': confidence_threshold, 'rpn_nms_thresh': rpn_nms,\n","                     'box_head': box_head, 'rpn_anchor_generator': anchor_generator, 'mask_head':None,\n","                     'mask_predictor': mask_predictor, 'box_predictor': box_predictor}\n","\n","    # Instantiate the segmentation model\n","    maskrcnn_model = mask_net.maskrcnn_resnet_fpn(backbone_name, truncation, pretrained_backbone=False, **maskrcnn_args)\n","    # Load weights\n","    maskrcnn_model.load_state_dict(ckpt['model_weights'])\n","    # Set to evaluation mode\n","    print(maskrcnn_model)\n","    maskrcnn_model.eval().to(device)\n","\n","    start_time = time.time()\n","    # get the correct masks and mask colors\n","    if mask_type == \"ggo\":\n","       ct_classes = {0: '__bgr', 1: 'GGO'}\n","       ct_colors = {1: 'red', 'mask_cols': np.array([[255, 0, 0]])}\n","    elif mask_type == \"merge\":\n","       ct_classes = {0: '__bgr', 1: 'Lesion'}\n","       ct_colors = {1: 'red', 'mask_cols': np.array([[255, 0, 0]])}\n","    elif mask_type == \"both\":\n","       ct_classes = {0: '__bgr', 1: 'GGO', 2: 'CL'}\n","       ct_colors = {1: 'red', 2: 'blue', 'mask_cols': np.array([[255, 0, 0], [0, 0, 255]])} \n","\n","    # run the inference with provided hyperparameters for images in the provided directory\n","    test_ims = os.listdir(os.path.join(data_dir, img_dir))\n","    \n","    for j, ims in enumerate(tqdm(test_ims)):\n","        test_step(os.path.join(os.path.join(data_dir, img_dir), ims), device, maskrcnn_model,confidence_threshold, mask_threshold, save_dir, ct_classes, ct_colors, j)\n","        if True: #REMOVE THIS AND BREAK LINE TO DO IT ON ALL IMAGES IN DIR\n","          break\n","    end_time = time.time()\n","    print(\"Inference took {0:.1f} seconds\".format(end_time - start_time))\n","    print(\"fig saved in \", save_dir)"],"metadata":{"id":"2dM7BtWsX8_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Here the image is passed through a convolutional network.\n","The output of first conv net, is passed through to a Region Proposal network (RPN) which creates different achor boxes (Regions of Interest) based on the presence of any of the objects to be detected.\n","The Anchor boxes are sent to ROI Align stage (one of the key features of Mask RCNN for protecting spatial orientation), which converts ROIâ€™s to the same size required for further processing\n","This output is sent to Fully connected layers which will generate the result of the class of the object in that specific region and the location of the bounding box for the object\n","The output of ROI Align stage is parallelly sent to Conv Nets in order to generate a mask of the pixels of the object but there are some hyperparameters constructing the model"],"metadata":{"id":"9QPtuazu57p9"}},{"cell_type":"code","source":["def test_step(image, device, model, theta_conf, theta_mask, save_dir, cls, cols, num):\n","    im = PILImage.open(image)\n","    # converts image to rgb\n","    if im.mode != 'RGB':\n","        im = im.convert(mode='RGB')\n","    img = np.array(im)\n","    # copy image to make background for plotting\n","    bgr_img = copy.deepcopy(img)\n","\n","    t_ = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n","    img = t_(img).to(device)\n","    out = model([img])\n","    # scores + bounding boxes + labels + masks\n","    scores = out[0]['scores']\n","    bboxes = out[0]['boxes']\n","    classes = out[0]['labels']\n","    mask = out[0]['masks']\n","    # this is the array for all masks\n","    best_scores = scores[scores > theta_conf]\n","    # Are there any detections with confidence above the threshold?\n","    if len(best_scores):\n","        best_idx = np.where(scores > theta_conf)\n","        best_bboxes = bboxes[best_idx]\n","        best_classes = classes[best_idx]\n","        best_masks = mask[best_idx]\n","        #print('bm', best_masks.shape)\n","        mask_array = np.zeros([best_masks[0].shape[1], best_masks[0].shape[2], 3], dtype=np.uint8)\n","        fig, ax = plt.subplots(1, 1)\n","        fig.set_size_inches(12, 6)\n","        ax.axis(\"off\")\n","        # plot predictions\n","        for idx, dets in enumerate(best_bboxes):\n","            found_masks = best_masks[idx][0].detach().clone().to(device).numpy()\n","            pred_class = best_classes[idx].item()\n","            pred_col_n = cols[pred_class]\n","            pred_class_txt = cls[pred_class]\n","            pred_col = cols['mask_cols'][pred_class - 1]\n","            mask_array[found_masks > theta_mask] = pred_col\n","            rect = Rectangle((dets[0], dets[1]), dets[2] - dets[0], dets[3] - dets[1], linewidth=1,edgecolor=pred_col_n, facecolor='none', linestyle=\"--\")\n","            ax.text(dets[0] + 40, dets[1], '{0:}'.format(pred_class_txt), fontsize=10, color=pred_col_n)\n","            ax.text(dets[0], dets[1], '{0:.2f}'.format(best_scores[idx]), fontsize=10, color=pred_col_n)\n","            ax.add_patch(rect)\n","\n","        added_image = cv2.addWeighted(bgr_img, 0.5, mask_array, 0.75, gamma=0)\n","        ax.imshow(added_image)\n","        #fig.savefig(os.path.join(save_dir, str(num) + \".png\"),bbox_inches='tight', pad_inches=0.0)\n","        fig.savefig(os.path.join(save_dir, \"__MASKED__\"+image.split('/')[-1] ),bbox_inches='tight', pad_inches=0.0)        \n","\n","    else:\n","        print(image, \" : No detections\")"],"metadata":{"id":"A5CwVL3VX8OR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. (last comment but the everything starts from here :D\n"," this is where we also need to define(config) the hyperparameters, which the most importatnt ones are:\n","\n","**Backbone**: The Backbone is the Conv Net architecture that is to be used in the first step of Mask R-CNN. one of the famous available options for choice of Backbones is  ResNet50. the choice of backbone should be based on the trade off between training time and accuracy. ResNet50 would take relatively lesser time than the later ones, and has several open source pre-trained weights for huge data sets like coco, which can considerably reduce the training time for different instance segmentation projects. ResNet 101 and ResNext 101 will take more time for training (because of the number of layers), but they tend to be more accurate if there are no pre-trained weights involved and basic parameters like **learning rate** and **number of epochs** are well tuned.\n","An ideal approach would be to start with pre-trained weights available like coco with ResNet 50 and evaluate the performance of the model. This would work faster and better on models which involve detection of real world objects which were trained in the coco dataset. If accuracy is of utmost importance and high computation power is available, the options other backbones can be explored.\n","\n","**Train_ROIs_Per_Image**\n","This is the maximum number of ROIâ€™s, the Region Proposal Network will generate for the image, which will further be processed for classification and masking in the next stage. The ideal way is to start with default values if number of instances in the image are unknown. If the number of instances are limited, it can be reduced to reduce the training time.\n","\n","**Max_GT_Instances:**\n","This is the maximum number of instances that can be detected in one image. If the number of instances in the images are limited, this can be set to maximum number of instances that can occur in the image. This helps in reduction of false positives and reduces the training time.\n","\n","**Detection_Min_Confidence:**\n","This is the confidence level threshold, beyond which the classification of an instance will happen. Initialization can be at default and reduced or increased based on the number of instances that are detected in the model. If detection of everything is important and false positives are fine(which is true in our case), reduce the threshold to identify every possible instance. If accuracy of detection is important, increase the threshold to ensure that there are minimal false positive by guaranteeing that the model predicts only the instances with very high confidence."],"metadata":{"id":"ce4RwucF7iNH"}},{"cell_type":"code","source":["# run the inference\n","if __name__ == '__main__':\n","    config_dict= {\n","        \"backbone_name\": \"reset50\",\n","        \"ckpt\": \"segmentation_folder/pretrained_model_for_instance_segmentation/segmentation_model_both_classes.pth\",\n","        \"confidence_th\": 0.05,   # theta conf\n","        \"device\": \"cpu\",\n","        \"gt_dir\": \"masks\",\n","        \"mask_logits_th\": 0.5,\n","        \"mask_type\": \"both\",\n","        \"model_name\": None,\n","        \"roi_nms_th\": 0.5,\n","        \"rpn_nms_th\": 0.75,\n","        \"test_data_dir\": \"segmentation_folder\", \n","        \"test_img_dir\": \"images\",\n","        \"save_dir\": \"segmentation_folder/results\",\n","        \"model_args\": None,\n","        \"truncation\": '0'\n","    }\n","    main(config_dict)"],"metadata":{"id":"NlIhh8psL0MV"},"execution_count":null,"outputs":[]}]}